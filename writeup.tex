\documentclass{article}
\title{COMP 424 - Hus AI Project Writeup}
\author{Kevin Li - 260565522}

\begin{document}
\maketitle
\newpage

\section{Notes}

Prepared by creating abstract classes to represent evaluation functions so that they can be easily swapped out
    in order to test different ones and see which would perform better. All evaluation functions would return a score normalized
    from 0 to 100, which was important because this way we could easily return scores of 100 for victory and 0 for loss/cancellation.

Began by implementing minimax with a very basic evaluation function that just used the difference in pits between my agent and
    the opponent. More pits = higher score for me.

In testing it turns out that this function would only search to a maximum depth of 4 on my machine;
    any higher depth would result in timeouts. It could beat the random agent around 90\% of the time, which was decent, but
    when searching to a depth of 4, only around 150ms would be used in computation.

Clearly a lot of capacity was being wasted, so I decided to implement alpha-beta pruning in order to further deepen the search.
    I noted that in minimax, I was constructing the entire tree first of depth N and then backing up the nodes of that tree. This would not
    be efficient in alpha-beta pruning, as it would be wasteful to construct a tree and include nodes that would only be pruned off;
    in other words, it would be ideal to forgo creating nodes (i.e. running moves) where we don't need to. My new implementation would
    require this.

After implementing alpha-beta pruning, I was able to search a bit deeper (specified at a depth of 5). I had also improved on the evaluation function
that I was using to take into account interior and exterior rows along with spaces that only contained one pit. But the depth parameter was exactly the shortcoming
of this approach; I had to specify a depth to which we should search, and if we weren't able to discover a move by the time we search to the specified depth then
the algorithm would never return a move in time.

After discussing with some colleagues I decided to implement iterative deepening in order to search to the deepest
possible degree and thus try and search as deep as possible each time. Of course, in order to prevent the agent from wasting time and repeatedly searching shallow depths,
it would need to start by searching to depth 4 or perhaps 5. I would also have to implement a way to determine a clever way of comparing the optimal move chosen from
a shallower search with an intermediate move in a deeper search (i.e. in the case where we have completed a search of depth n, and the thread is terminated in the middle
of searching in depth n+1). Further care would have to be taken in order to avoid as much as possible re-searching through already searched nodes.

Through observing my agent I noticed that towards the ends of games it would not take a move that would result in its immediate victory, and thus
games would break down into these long torturous sessions of the agent chasing down opponent pits. It was through this observation that
I discovered that in my alpha-beta pruning recursive function I was not taking into account whether the current state was a victory or loss state,
which resulted in the agent searching beyond these terminal states. After having corrected this, along with lowering the computational overhead by reducing
the use of unnecessary data structures (mainly Iterators), I was able to vastly reduce the number of turns it took for me to achieve victory against 
the random agent!

I then noticed an additional problem. As I was using iterative deepening, I was making the decision to throw away the previously obtained optimal solution from the
previous depth of search and ended up using the newly generated move from the deeper depth, which theoretically should mean that the solution is better. But if the thread
is interrupted before that new depth of search is completed, it is likely (and happens very often) that the 'new' solution is sub-optimal. I tried fixing this by properly ordering
the board states that I would analyze, and the number of branches that got pruned rose dramatically.

Later on, I played my agent several times and managed to win using a strategy that focused on capturing as many pieces as possible. Since I was able to beat my agent, it
did bode well for my agent and I thought that I should encode my strategy into an evaluation function.

My agent was still inefficient because when it searched through depth X and found several moves (improving upon moves that were previously thought to be optimal but were not)
and then moved on to depth X+1, it would look through those same suboptimal moves in the same order. This meant that occasionally when the thread terminates at the time
limit, the agent might consider a move that is known to be sub-optimal as optimal and return that move. In order to fix this, when an optimal move is discovered, the agent
will re-order the states through which it searches at the top level. This makes it search the optimal move first, fixing this problem of choosing a suboptimal move first.
Of course, if a previously suboptimal move turns out to be better at a deeper depth, this agent will still find it as it is still optimal because it will end up searching
through the same moves, just in a better order.

Implemented Monte Carlo Tree Search with a heavy rollout policy. It performed far worse than minimax with alpha-beta pruning.

After ordering all possible states, try only exploring first X number of moves. Maybe square root of number of moves if there are more than 16. Or cap it at some number.(should not be good)

\end{document}
